import numpy as np
import torch
import random
from envs import RestaurantEnv
from agents import DQNAgent, VDNAgent, SharedReplayBuffer
from agents.tar2 import TAR2Network, collate_trajectories
from .curriculum import Curriculum

class Trainer:
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆDQN/VDN + TAR2 + é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ  å¯¾å¿œï¼‰"""
    
    # ä¿®æ­£: use_tar2 ã‚’å¼•æ•°ã«è¿½åŠ 
    def __init__(self, num_episodes=30000, use_shared_replay=True, use_vdn=False, use_tar2=False, config=None):
        self.num_episodes = num_episodes
        self.use_shared_replay = use_shared_replay
        self.use_vdn = use_vdn
        self.use_tar2 = use_tar2  # ä¿®æ­£: å¼•æ•°ã‹ã‚‰ç›´æ¥è¨­å®š
        self.config = config
        
        # é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚’ä½¿ç”¨
        self.curriculum = Curriculum()
        
        self.agents = {}
        self.episode_rewards = {}
        self.avg_rewards = {}
        self.served_stats = {}

        # TAR2ç”¨ãƒãƒƒãƒ•ã‚¡ (è¿½åŠ )
        self.tar2 = None
        self.tar2_buffer = []
    
    def train(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        action_dim = 4
        
        # --- åˆæœŸç’°å¢ƒè¨­å®šï¼ˆçŠ¶æ…‹æ¬¡å…ƒå–å¾—ç”¨ï¼‰ ---
        # [cite_start]é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‹ã‚‰åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å–å¾— [cite: 68]
        current_stage = self.curriculum.get_current_stage()
        
        # ç’°å¢ƒåˆæœŸåŒ–
        current_env = RestaurantEnv(
            layout_type=current_stage['layout'],
            enable_customers=current_stage['customers'],
            customer_spawn_interval=current_stage['spawn_interval'],
            local_obs_size=5,
            config=self.config
        )

        state_dim = current_env.observation_space('agent_0').shape[0]
        print(f"State Dimension: {state_dim}")
        print(f"System: {'VDN' if self.use_vdn else 'Independent DQN'} | TAR2: {'ON' if self.use_tar2 else 'OFF'}")
        
        # TAR2 åˆæœŸåŒ–
        if self.use_tar2:
            self.tar2 = TAR2Network(state_dim, action_dim, num_agents=2)
            self.tar2_buffer = []

        # ãƒãƒƒãƒ•ã‚¡ãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        shared_buffer = SharedReplayBuffer(capacity=50000) if self.use_shared_replay else None
        
        if self.use_vdn:
            self.agents = {
                'vdn': VDNAgent(state_dim, action_dim, num_agents=2, shared_buffer=shared_buffer)
            }
        else:
            self.agents = {
                agent_name: DQNAgent(state_dim, action_dim, shared_buffer=shared_buffer) 
                for agent_name in current_env.possible_agents
            }
        
        self.episode_rewards = {agent: [] for agent in current_env.possible_agents}
        self.avg_rewards = {agent: [] for agent in current_env.possible_agents}
        self.served_stats = {agent: [] for agent in current_env.possible_agents}
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸æ»åœ¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        stage_episode_count = 0
        
        print(f"\n{'='*70}")
        print(f"=== STARTING STAGE: {current_stage['description']} ===")
        print(f"{'='*70}")
        
        for episode in range(self.num_episodes):
            
            # ----------------------------------------------------
            # 0. é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã®é€²è¡Œåˆ¤å®š (é–¾å€¤/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)
            # ----------------------------------------------------
            # agent_0 ã®å¹³å‡å ±é…¬ã‚’ä»£è¡¨å€¤ã¨ã—ã¦ä½¿ç”¨
            current_served_performance = 0
            if len(self.served_stats['agent_0']) > 0:
                recent_indices = range(-min(50, len(self.served_stats['agent_0'])), 0)
                # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆè¨ˆã™ã‚‹
                total_served_list = [
                    sum(self.served_stats[agent][i] for agent in current_env.possible_agents)
                    for i in recent_indices
                ]
                current_served_performance = np.mean(total_served_list)
            else:
                current_served_performance = 0

            # check_progression ã«é…è†³æ•°ã®å¹³å‡ã‚’æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´ [cite: 143]
            should_proceed, reason = self.curriculum.check_progression(
                current_served_performance, 
                stage_episode_count
            )

            if should_proceed:
                # æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€
                new_stage = self.curriculum.get_current_stage()
                
                print(f"\n{'='*70}")
                print(f"ğŸ”„ CURRICULUM PROGRESSION")
                print(f"   From: {current_stage['description']}")
                print(f"   To:   {new_stage['description']}")
                print(f"   Why:  {reason}")
                print(f"   Perf: {current_served_performance:.1f} (Target: {current_stage['threshold']})")
                print(f"{'='*70}")
                
                # [cite_start]æ–°ã—ã„ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®šã§ç’°å¢ƒã‚’å†æ§‹ç¯‰ [cite: 79]
                current_stage = new_stage
                current_env = RestaurantEnv(
                    layout_type=current_stage['layout'],
                    enable_customers=current_stage['customers'],
                    customer_spawn_interval=current_stage['spawn_interval'],
                    local_obs_size=5,
                    coop_factor=0.5,
                    config=self.config
                )
                
                # æ»åœ¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
                stage_episode_count = 0
                
                # æ¢ç´¢ç‡(epsilon)ã®ãƒªã‚»ãƒƒãƒˆï¼ˆç’°å¢ƒãŒå¤‰ã‚ã£ãŸã®ã§å†æ¢ç´¢ã•ã›ã‚‹ï¼‰
                reset_epsilon = 0.6
                if self.use_vdn:
                    self.agents['vdn'].epsilon = max(self.agents['vdn'].epsilon, reset_epsilon)
                else:
                    for agent_name in self.agents:
                        self.agents[agent_name].epsilon = max(self.agents[agent_name].epsilon, reset_epsilon)

            # ----------------------------------------------------
            # 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º (å­¦ç¿’ãªã—ã§èµ°ã‚‰ã›ã‚‹)
            # ----------------------------------------------------
            trajectory_data = self._run_episode_collect_only(current_env)
            stage_episode_count += 1
            
            # å ±é…¬è¨˜éŒ²
            total_r = trajectory_data['total_reward']
            for agent_name in current_env.possible_agents:
                self.episode_rewards[agent_name].append(total_r / 2)
                self.avg_rewards[agent_name].append(np.mean(self.episode_rewards[agent_name][-50:]))
                self.served_stats[agent_name].append(current_env.served_count[agent_name])

            # ----------------------------------------------------
            # 2. TAR2 å ±é…¬å†è¨ˆç®—ãƒ•ã‚§ãƒ¼ã‚º
            # ----------------------------------------------------
            shaped_rewards = None
            if self.use_tar2:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                self.tar2_buffer.append(trajectory_data)
                
                # TAR2ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ (ãƒãƒƒãƒãŒæºœã¾ã£ãŸã‚‰)
                if len(self.tar2_buffer) >= 32:
                    b_states, b_actions, b_rewards, _ = collate_trajectories(self.tar2_buffer, self.tar2.device)
                    tar2_loss = self.tar2.update(b_states, b_actions, b_rewards)
                    self.tar2_buffer = []

                # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å ±é…¬ã‚’å†åˆ†é… (æ¨è«–)
                s, a, r_tot, _ = collate_trajectories([trajectory_data], self.tar2.device)
                f_s = s[:, -1, :, :]
                
                with torch.no_grad():
                    scores, _ = self.tar2(s, a, f_s)
                    shaped_tensor = self.tar2.get_redistributed_rewards(scores, r_tot)
                    shaped_rewards = shaped_tensor.squeeze(0).cpu().numpy() # (T, N)
            
            # ----------------------------------------------------
            # 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º (å†è¨ˆç®—ã•ã‚ŒãŸå ±é…¬ã‚’ä½¿ç”¨)
            # ----------------------------------------------------
            self._store_and_train_agents(trajectory_data, shaped_rewards)

            # Epsilonæ¸›è¡°
            if self.use_vdn:
                self.agents['vdn'].decay_epsilon()
            else:
                for agent_name in current_env.possible_agents:
                    self.agents[agent_name].decay_epsilon()
            
            # [cite_start]å®šæœŸæ›´æ–° [cite: 90]
            if episode % 10 == 0:
                if self.use_vdn:
                    self.agents['vdn'].update_target_network()
                else:
                    for agent_name in current_env.possible_agents:
                        self.agents[agent_name].update_target_network()
            
            # ãƒ­ã‚°è¡¨ç¤º
            if episode % 100 == 0:
                # 1. å¹³å‡å ±é…¬ï¼ˆç›´è¿‘50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç§»å‹•å¹³å‡ï¼‰ã®å–å¾— 
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã®å¹³å‡å ±é…¬ã‚’åˆè¨ˆã—ã¦ãƒãƒ¼ãƒ å…¨ä½“ã®æˆæœã¨ã™ã‚‹
                avg_0 = self.avg_rewards['agent_0'][-1] if self.avg_rewards['agent_0'] else 0
                avg_1 = self.avg_rewards['agent_1'][-1] if self.avg_rewards['agent_1'] else 0
                team_avg_reward = avg_0 + avg_1

                # 2. é…è†³æ•°ã®çµ±è¨ˆï¼ˆç›´è¿‘50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ 
                served_a0 = np.mean(self.served_stats['agent_0'][-50:])
                served_a1 = np.mean(self.served_stats['agent_1'][-50:])
                total_served = served_a0 + served_a1
                
                # 3. æ¢ç´¢ç‡ã¨TAR2ã®çŠ¶æ…‹å–å¾—
                eps = self.agents['vdn'].epsilon if self.use_vdn else self.agents['agent_0'].epsilon
                tar2_msg = " | TAR2 Shaped" if self.use_tar2 else ""
                
                # ãƒ­ã‚°è¡¨ç¤ºã®æ›´æ–°
                print(f"Ep {episode:4d} | StgEp: {stage_episode_count:4d} | "
                      f"AvgReward: {team_avg_reward:6.1f} | "  # â† ã“ã“ã«å¾©æ´»ã•ã›ã¾ã—ãŸ
                      f"Total Served: {total_served:4.1f} (A0:{served_a0:.1f}, A1:{served_a1:.1f}) | "
                      f"Îµ={eps:.3f}{tar2_msg}")
        
        return self.agents, self.episode_rewards, self.avg_rewards, self.served_stats, current_env

    def _run_episode_collect_only(self, env):
        """å­¦ç¿’ã‚’è¡Œã‚ãšã€å…¨ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦è¿”ã™"""
        env.reset()
        
        states_seq = []
        actions_seq = []
        rewards_seq = []
        dones_seq = []
        
        states = {agent: env.observe(agent) for agent in env.possible_agents}
        
        # ä¿®æ­£: ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å¤‰æ›´ã«ä¼´ã„ã€ãƒ©ãƒ³ãƒ€ãƒ æ³¨æ–‡ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯å‰Šé™¤ã—ã¾ã—ãŸ
        
        episode_reward_sum = 0
        agents_order = env.possible_agents 
        
        for step in range(600):
            step_states = []
            step_actions = []
            step_rewards = []
            step_dones = []
            
            current_actions = {}
            for agent_name in agents_order:
                state = states[agent_name]
                step_states.append(state)
                
                if env.truncations.get(agent_name, False):
                    action = 0
                else:
                    if self.use_vdn:
                        actions_dict = self.agents['vdn'].select_actions(states)
                        action = actions_dict[agent_name]
                    else:
                        action = self.agents[agent_name].select_action(state)
                
                current_actions[agent_name] = action
                step_actions.append(action)

            for agent_name in agents_order:
                if env.agent_selection == agent_name:
                    env.step(current_actions[agent_name])
            
            for agent_name in agents_order:
                next_obs = env.observe(agent_name)
                states[agent_name] = next_obs
                
                r = env.rewards.get(agent_name, 0)
                d = env.truncations.get(agent_name, False)
                
                step_rewards.append(r)
                step_dones.append(d)
                
                episode_reward_sum += r

            states_seq.append(np.array(step_states))
            actions_seq.append(np.array(step_actions))
            rewards_seq.append(np.array(step_rewards))
            dones_seq.append(np.array(step_dones))

            if all(env.truncations.values()):
                break
        
        return {
            'states': np.array(states_seq),
            'actions': np.array(actions_seq),
            'rewards': np.array(rewards_seq),
            'dones': np.array(dones_seq),
            'total_reward': episode_reward_sum
        }

    def _store_and_train_agents(self, trajectory, shaped_rewards):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨å ±é…¬ã‚’ä½¿ã£ã¦ãƒãƒƒãƒ•ã‚¡ä¿å­˜ã¨å­¦ç¿’ã‚’è¡Œã†"""
        T = len(trajectory['states'])
        agents_order = ['agent_0', 'agent_1']
        
        for t in range(T - 1):
            s_t = trajectory['states'][t]
            a_t = trajectory['actions'][t]
            ns_t = trajectory['states'][t+1]
            d_t = trajectory['dones'][t]
            
            if shaped_rewards is not None:
                r_t = shaped_rewards[t]
            else:
                r_t = trajectory['rewards'][t]

            s_dict = {name: s_t[i] for i, name in enumerate(agents_order)}
            a_dict = {name: a_t[i] for i, name in enumerate(agents_order)}
            r_dict = {name: r_t[i] for i, name in enumerate(agents_order)}
            ns_dict = {name: ns_t[i] for i, name in enumerate(agents_order)}
            d_dict = {name: bool(d_t[i]) for i, name in enumerate(agents_order)}
            
            if self.use_vdn:
                self.agents['vdn'].store_transition(s_dict, a_dict, r_dict, ns_dict, d_dict)
                # ã”è¦æœ›é€šã‚Šã€æ¯ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’ã‚’è¡Œã†å…ƒã®ä»•æ§˜ã‚’ç¶­æŒã—ã¦ã„ã¾ã™
                self.agents['vdn'].train()
            else:
                for i, name in enumerate(agents_order):
                    self.agents[name].store_transition(
                        s_dict[name], a_dict[name], r_dict[name], ns_dict[name], d_dict[name]
                    )
                    # ã”è¦æœ›é€šã‚Šã€æ¯ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’ã‚’è¡Œã†å…ƒã®ä»•æ§˜ã‚’ç¶­æŒã—ã¦ã„ã¾ã™
                    self.agents[name].train()