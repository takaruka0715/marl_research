import numpy as np
import torch
import random
from envs import RestaurantEnv
from agents import DQNAgent, VDNAgent, SharedReplayBuffer
from agents.tar2 import TAR2Network, collate_trajectories
from .curriculum import Curriculum
from agents.qmix import QMIXAgent

class Trainer:
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆParallelEnvå¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self, num_episodes=30000, use_shared_replay=True, use_qmix=False, use_vdn=False, use_tar2=False, config=None):
        self.num_episodes = num_episodes
        self.use_shared_replay = use_shared_replay
        self.use_vdn = use_vdn
        self.use_tar2 = use_tar2
        self.use_qmix = use_qmix
        self.config = config
        
        # é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ 
        self.curriculum = Curriculum()
        
        self.agents = {}
        self.episode_rewards = {}
        self.avg_rewards = {}
        self.served_stats = {}

        # TAR2ç”¨ãƒãƒƒãƒ•ã‚¡
        self.tar2 = None
        self.tar2_buffer = []
    
    def train(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        action_dim = 5 # ParallelEnvã«å¤‰æ›´ã—ãŸãŸã‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¬¡å…ƒã‚’ç¢ºèª (Waitå«ã‚€ãªã‚‰5ã€ç§»å‹•ã®ã¿ãªã‚‰4)
                       # â€»Envå´ã§ spaces.Discrete(5) ã«è¨­å®šã—ãŸãŸã‚ 5 ã«ä¿®æ­£
        
        # åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¸å–å¾—
        current_stage = self.curriculum.get_current_stage()
        
        # ç’°å¢ƒåˆæœŸåŒ–
        current_env = RestaurantEnv(
            layout_type=current_stage['layout'],
            enable_customers=current_stage['customers'],
            customer_spawn_interval=current_stage['spawn_interval'],
            local_obs_size=5,
            config=self.config
        )

        state_dim = current_env.observation_space('agent_0').shape[0]

        algo_name = "Independent DQN"
        if self.use_qmix: algo_name = "QMIX"
        elif self.use_vdn: algo_name = "VDN"

        print(f"State Dimension: {state_dim}")
        print(f"System: {algo_name} | TAR2: {'ON' if self.use_tar2 else 'OFF'}")
        
        # TAR2 åˆæœŸåŒ–
        if self.use_tar2:
            self.tar2 = TAR2Network(state_dim, action_dim, num_agents=2)
            self.tar2_buffer = []

        # ãƒãƒƒãƒ•ã‚¡ãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        shared_buffer = SharedReplayBuffer(capacity=50000) if self.use_shared_replay else None
        
        if self.use_qmix:
            global_state_dim = state_dim * 2 
            self.agents = {
                'qmix': QMIXAgent(state_dim, action_dim, global_state_dim, num_agents=2, shared_buffer=shared_buffer)
            }
        elif self.use_vdn:
            self.agents = {
                'vdn': VDNAgent(state_dim, action_dim, num_agents=2, shared_buffer=shared_buffer)
            }
        else:
            self.agents = {
                agent_name: DQNAgent(state_dim, action_dim, shared_buffer=shared_buffer) 
                for agent_name in current_env.possible_agents
            }
        
        self.episode_rewards = {agent: [] for agent in current_env.possible_agents}
        self.avg_rewards = {agent: [] for agent in current_env.possible_agents}
        self.served_stats = {agent: [] for agent in current_env.possible_agents}
        
        stage_episode_count = 0
        
        print(f"\n{'='*70}")
        print(f"=== STARTING STAGE: {current_stage['description']} ===")
        print(f"{'='*70}")
        
        for episode in range(self.num_episodes):
            
            # --- ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ é€²è¡Œåˆ¤å®š ---
            current_served_performance = 0
            if len(self.served_stats['agent_0']) > 0:
                recent_indices = range(-min(50, len(self.served_stats['agent_0'])), 0)
                total_served_list = [
                    sum(self.served_stats[agent][i] for agent in current_env.possible_agents)
                    for i in recent_indices
                ]
                current_served_performance = np.mean(total_served_list)

            should_proceed, reason = self.curriculum.check_progression(
                current_served_performance, 
                stage_episode_count
            )

            if should_proceed:
                new_stage = self.curriculum.get_current_stage()
                print(f"\n{'='*70}")
                print(f"ğŸ”„ CURRICULUM PROGRESSION")
                print(f"   From: {current_stage['description']}")
                print(f"   To:   {new_stage['description']}")
                print(f"   Why:  {reason}")
                print(f"   Perf: {current_served_performance:.1f}")
                print(f"{'='*70}")
                
                current_stage = new_stage
                current_env = RestaurantEnv(
                    layout_type=current_stage['layout'],
                    enable_customers=current_stage['customers'],
                    customer_spawn_interval=current_stage['spawn_interval'],
                    local_obs_size=5,
                    coop_factor=self.config.coop_factor,
                    config=self.config
                )
                stage_episode_count = 0
                reset_epsilon = 0.8
                
                if self.use_vdn:
                    self.agents['vdn'].epsilon = max(self.agents['vdn'].epsilon, reset_epsilon)
                elif getattr(self, 'use_qmix', False):
                    self.agents['qmix'].epsilon = max(self.agents['qmix'].epsilon, reset_epsilon)
                else:
                    for agent_name in self.agents:
                        self.agents[agent_name].epsilon = max(self.agents[agent_name].epsilon, reset_epsilon)

            # --- 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º (ParallelEnvå¯¾å¿œ) ---
            trajectory_data = self._run_episode_collect_only(current_env)
            stage_episode_count += 1
            
            # å ±é…¬è¨˜éŒ²
            total_r = trajectory_data['total_reward']
            for agent_name in current_env.possible_agents:
                # ParallelEnvã§ã¯å€‹åˆ¥ã«å ±é…¬ãŒå‡ºã‚‹ãŒã€ãƒãƒ¼ãƒ è©•ä¾¡ç”¨ã«åˆè¨ˆã‚’2åˆ†å‰²ã—ã¦è¨˜éŒ²ï¼ˆå¾“æ¥äº’æ›ï¼‰
                # ã‚‚ã—ãã¯ trajectory['rewards'] ã‹ã‚‰å€‹åˆ¥ã®åˆè¨ˆã‚’å‡ºã™ã®ãŒæ­£ç¢º
                agent_idx = current_env.possible_agents.index(agent_name)
                agent_total_r = np.sum(trajectory_data['rewards'][:, agent_idx])
                
                self.episode_rewards[agent_name].append(agent_total_r)
                self.avg_rewards[agent_name].append(np.mean(self.episode_rewards[agent_name][-50:]))
                self.served_stats[agent_name].append(current_env.served_count[agent_name])

            # --- 2. TAR2 å ±é…¬å†è¨ˆç®—ãƒ•ã‚§ãƒ¼ã‚º ---
            shaped_rewards = None
            if self.use_tar2:
                self.tar2_buffer.append(trajectory_data)
                
                if len(self.tar2_buffer) >= 32:
                    b_states, b_actions, b_rewards, _ = collate_trajectories(self.tar2_buffer, self.tar2.device)
                    self.tar2.update(b_states, b_actions, b_rewards)
                    self.tar2_buffer = []

                s, a, r_tot, _ = collate_trajectories([trajectory_data], self.tar2.device)
                f_s = s[:, -1, :, :]
                
                with torch.no_grad():
                    scores, _ = self.tar2(s, a, f_s)
                    shaped_tensor = self.tar2.get_redistributed_rewards(scores, r_tot)
                    shaped_rewards = shaped_tensor.squeeze(0).cpu().numpy() # (T, N)
            
            # --- 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º ---
            self._store_and_train_agents(trajectory_data, shaped_rewards)

            # Epsilonæ¸›è¡°
            if self.use_vdn:
                self.agents['vdn'].decay_epsilon()
            elif getattr(self, 'use_qmix', False):
                self.agents['qmix'].decay_epsilon()
            else:
                for agent_name in current_env.possible_agents:
                    self.agents[agent_name].decay_epsilon()
            
            # Target Networkæ›´æ–°
            if episode % 10 == 0:
                if self.use_vdn:
                    self.agents['vdn'].update_target_network()
                elif getattr(self, 'use_qmix', False):
                    self.agents['qmix'].update_target_network()
                else:
                    for agent_name in current_env.possible_agents:
                        self.agents[agent_name].update_target_network()
            
            # ãƒ­ã‚°è¡¨ç¤º
            if episode % 100 == 0:
                avg_0 = self.avg_rewards['agent_0'][-1] if self.avg_rewards['agent_0'] else 0
                avg_1 = self.avg_rewards['agent_1'][-1] if self.avg_rewards['agent_1'] else 0
                team_avg_reward = avg_0 + avg_1

                served_a0 = np.mean(self.served_stats['agent_0'][-50:])
                served_a1 = np.mean(self.served_stats['agent_1'][-50:])
                total_served = served_a0 + served_a1
                
                if self.use_vdn:
                    eps = self.agents['vdn'].epsilon
                elif getattr(self, 'use_qmix', False):
                    eps = self.agents['qmix'].epsilon
                else:
                    eps = self.agents['agent_0'].epsilon # Independentã®å ´åˆã¯ä»£è¡¨è¡¨ç¤º
                
                tar2_msg = " | TAR2 Shaped" if self.use_tar2 else ""
                
                print(f"Ep {episode:4d} | StgEp: {stage_episode_count:4d} | "
                      f"AvgReward: {team_avg_reward:6.1f} | "
                      f"Total Served: {total_served:4.1f} (A0:{served_a0:.1f}, A1:{served_a1:.1f}) | "
                      f"Îµ={eps:.3f}{tar2_msg}")
        
        return self.agents, self.episode_rewards, self.avg_rewards, self.served_stats, current_env

    def _run_episode_collect_only(self, env):
        """
        ParallelEnvç”¨ã®ãƒ‡ãƒ¼ã‚¿åé›†ãƒ«ãƒ¼ãƒ—
        å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒæ™‚ã«è¡Œå‹•æ±ºå®šãƒ»å®Ÿè¡Œã‚’è¡Œã†
        """
        observations, infos = env.reset()
        
        states_seq = []
        actions_seq = []
        rewards_seq = []
        dones_seq = []
        global_states_seq = [] 

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDã®å›ºå®šé †åºï¼ˆä¿å­˜ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã®ãŸã‚ï¼‰
        # ParallelEnvã§ã¯ dict ã§è¿”ã‚‹ãŸã‚ã€é †åºã‚’ä¿è¨¼ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        agent_ids = env.possible_agents # ['agent_0', 'agent_1']
        
        def get_global_state(obs_dict):
            # è¾æ›¸ã‹ã‚‰ãƒªã‚¹ãƒˆé †ã«è¦³æ¸¬ã‚’å–ã‚Šå‡ºã—ã¦çµåˆ
            return np.concatenate([obs_dict[aid] for aid in agent_ids])

        current_global_state = get_global_state(observations)
        episode_reward_sum = 0
        
        # ParallelEnv ã®ãƒ«ãƒ¼ãƒ—
        while env.agents: # PettingZoo: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ®‹ã£ã¦ã„ã‚‹é™ã‚Šãƒ«ãƒ¼ãƒ—
            step_actions_dict = {}
            step_states_list = []
            
            # 1. å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•æ±ºå®š (åŒæ™‚)
            # ParallelEnvãªã®ã§ã€ã“ã®æ™‚ç‚¹ã§ã® observations ã¯å…¨å“¡ã€Œç§»å‹•å‰ã€ã®çŠ¶æ…‹
            
            # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            for agent_name in agent_ids:
                # çµ‚äº†ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ observations ã«å«ã¾ã‚Œãªã„å ´åˆãŒã‚ã‚‹
                if agent_name in observations:
                    step_states_list.append(observations[agent_name])
                else:
                    # æ—¢ã«Doneã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: ã‚¼ãƒ­åŸ‹ã‚ãªã©ã§å¯¾å‡¦ã™ã‚‹ã‹ã€
                    # ã“ã“ã§ã¯ç°¡å˜ã®ãŸã‚ç›´å‰ã®å€¤ã‚’ä¿æŒã™ã‚‹ã‹ã€ã‚‚ã—ãã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚
                    # é€šå¸¸ã€env.agents ã«å…¥ã£ã¦ã„ãªã‘ã‚Œã°è¡Œå‹•ä¸è¦ã€‚
                    pass

            if self.use_vdn:
                # VDN: ã¾ã¨ã‚ã¦è¡Œå‹•é¸æŠ
                # observations ã¯ {agent_id: obs} ãªã®ã§ãã®ã¾ã¾æ¸¡ã›ã‚‹
                step_actions_dict = self.agents['vdn'].select_actions(observations)
            elif getattr(self, 'use_qmix', False):
                step_actions_dict = self.agents['qmix'].select_actions(observations)
            else:
                # Independent DQN
                for agent_name in env.agents:
                    step_actions_dict[agent_name] = self.agents[agent_name].select_action(observations[agent_name])
            
            # 2. ç’°å¢ƒã‚’1ã‚¹ãƒ†ãƒƒãƒ—é€²ã‚ã‚‹ (åŒæ™‚æ›´æ–°)
            next_observations, rewards, terminations, truncations, infos = env.step(step_actions_dict)
            
            # 3. ãƒ‡ãƒ¼ã‚¿ä¿å­˜ (ãƒªã‚¹ãƒˆé †åºã‚’æƒãˆã‚‹)
            # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’1è¡Œã®é…åˆ—ã¨ã—ã¦ä¿å­˜ã™ã‚‹
            
            # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿
            s_row = []
            a_row = []
            r_row = []
            d_row = []
            
            for agent_name in agent_ids:
                # çŠ¶æ…‹
                if agent_name in observations:
                    s_row.append(observations[agent_name])
                else:
                    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã„ãªããªã£ãŸå ´åˆï¼ˆé€”ä¸­é›¢è„±ï¼‰ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã‚‹ã‹ã€
                    # æœ€å¾Œã®è¦³æ¸¬ã‚’ä½¿ã†ã€‚ã“ã“ã§ã¯å‰ã®ãƒ«ãƒ¼ãƒ—ã§ç¢ºä¿ã—ãŸshapeã«åˆã‚ã›ã¦ã‚¼ãƒ­åŸ‹ã‚æ¨å¥¨ã ãŒ
                    # ç°¡æ˜“çš„ã« zeros ã‚’å…¥ã‚Œã‚‹
                    s_dim = env.observation_space(agent_name).shape[0]
                    s_row.append(np.zeros(s_dim))

                # è¡Œå‹•
                if agent_name in step_actions_dict:
                    a_row.append(step_actions_dict[agent_name])
                else:
                    a_row.append(0) # No-op or Dummy
                
                # å ±é…¬
                r = rewards.get(agent_name, 0.0)
                r_row.append(r)
                episode_reward_sum += r
                
                # çµ‚äº†åˆ¤å®š
                term = terminations.get(agent_name, False)
                trunc = truncations.get(agent_name, False)
                d_row.append(term or trunc)

            global_states_seq.append(current_global_state)
            states_seq.append(np.array(s_row))
            actions_seq.append(np.array(a_row))
            rewards_seq.append(np.array(r_row))
            dones_seq.append(np.array(d_row))
            
            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
            observations = next_observations
            
            # global state æ›´æ–° (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ¸›ã£ã¦ã‚‚ shape ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã®å·¥å¤«ãŒå¿…è¦ã ãŒã€
            # ä»Šå›ã®ç’°å¢ƒã¯å…¨å“¡åŒæ™‚ã«çµ‚ã‚ã‚‹è¨­å®šãªã®ã§ã€observations ã‹ã‚‰å†æ§‹ç¯‰ã§OK)
            # ãŸã ã—ã€å…¨å“¡çµ‚äº†ã—ã¦ empty ã«ãªã£ãŸå ´åˆã¯ next_obs ãŒç©ºã«ãªã‚‹
            if env.agents:
                current_global_state = get_global_state(observations)
            else:
                # çµ‚äº†æ™‚ã¯æ¬¡çŠ¶æ…‹ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã¯ä¸è¦ï¼ˆã‚ã‚‹ã„ã¯å…¨éƒ¨0ï¼‰
                # store_transition ã§ next_state ã‚’ä½¿ã†ãŸã‚ã€ä¾¿å®œä¸Šæœ€å¾Œã® state ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã‹ã‚¼ãƒ­åŸ‹ã‚
                pass

        # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã®å‡¦ç†
        # store_and_train ã§ next_state ã‚’ä½¿ã†ãŸã‚ã€æœ€å¾Œã® next_state ã‚‚ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
        # é…åˆ—é•·ã‚’åˆã‚ã›ã‚‹ãŸã‚ã€å‘¼ã³å‡ºã—å…ƒã§ t ã¨ t+1 ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ä½¿ã†ã€‚
        # ã“ã“ã§ã¯ states_seq ã«ã¯ T ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã® s_t ãŒå…¥ã£ã¦ã„ã‚‹ã€‚
        # T+1 å€‹ç›®ã® s_{T} (Terminal State) ã‚’è¿½åŠ ã—ã¦ãŠã
        
        # çµ‚äº†æ™‚ã®è¦³æ¸¬ï¼ˆã‚¼ãƒ­åŸ‹ã‚ or æœ€å¾Œã®obsï¼‰ã‚’ä½œæˆ
        final_s_row = []
        for agent_name in agent_ids:
            # å®Œå…¨ã«çµ‚äº†ã—ã¦ã„ã‚‹ã®ã§ env.agents ã¯ç©ºã€‚
            # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
            s_dim = env.observation_space(agent_name).shape[0]
            final_s_row.append(np.zeros(s_dim))
        states_seq.append(np.array(final_s_row))
        
        # global state ã‚‚åŒæ§˜ã«æœ€å¾Œã‚’è¿½åŠ 
        s_dim = env.observation_space(agent_ids[0]).shape[0]
        final_global = np.zeros(s_dim * len(agent_ids))
        global_states_seq.append(final_global)

        return {
            'states': np.array(states_seq),         # (T+1, N, Dim)
            'actions': np.array(actions_seq),       # (T, N)
            'rewards': np.array(rewards_seq),       # (T, N)
            'dones': np.array(dones_seq),           # (T, N)
            'global_states': np.array(global_states_seq), # (T+1, GlobalDim)
            'total_reward': episode_reward_sum
        }

    def _store_and_train_agents(self, trajectory, shaped_rewards):
        """ãƒãƒƒãƒ•ã‚¡ä¿å­˜ã¨å­¦ç¿’ (å¤‰æ›´ãªã—ãƒ»ãŸã ã—è¾æ›¸ä½œæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ç¢ºèª)"""
        # states_seq ã®é•·ã•ã¯ T+1, actions_seq ã¯ T
        T = len(trajectory['actions'])
        agent_ids = ['agent_0', 'agent_1']
        
        for t in range(T):
            s_t = trajectory['states'][t]
            a_t = trajectory['actions'][t]
            ns_t = trajectory['states'][t+1]
            d_t = trajectory['dones'][t]

            g_s_t = trajectory['global_states'][t]
            g_ns_t = trajectory['global_states'][t+1]
            
            if shaped_rewards is not None:
                r_t = shaped_rewards[t]
            else:
                r_t = trajectory['rewards'][t]

            s_dict = {name: s_t[i] for i, name in enumerate(agent_ids)}
            a_dict = {name: a_t[i] for i, name in enumerate(agent_ids)}
            r_dict = {name: r_t[i] for i, name in enumerate(agent_ids)}
            ns_dict = {name: ns_t[i] for i, name in enumerate(agent_ids)}
            d_dict = {name: bool(d_t[i]) for i, name in enumerate(agent_ids)}

            if getattr(self, 'use_qmix', False):
                self.agents['qmix'].store_transition(s_dict, a_dict, r_dict, ns_dict, d_dict, g_s_t, g_ns_t)
                self.agents['qmix'].train()
            elif self.use_vdn:
                self.agents['vdn'].store_transition(s_dict, a_dict, r_dict, ns_dict, d_dict)
                self.agents['vdn'].train()
            else:
                for i, name in enumerate(agent_ids):
                    self.agents[name].store_transition(
                        s_dict[name], a_dict[name], r_dict[name], ns_dict[name], d_dict[name]
                    )
                    self.agents[name].train()