import numpy as np
import torch
import random
from envs import RestaurantEnv
from agents import DQNAgent, VDNAgent, SharedReplayBuffer
from agents.tar2 import TAR2Network, collate_trajectories
from .curriculum import Curriculum
from agents.qmix import QMIXAgent

class Trainer:
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆDQN/VDN + TAR2 + é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ  å¯¾å¿œï¼‰"""
    
    # ä¿®æ­£: use_tar2 ã‚’å¼•æ•°ã«è¿½åŠ 
    def __init__(self, num_episodes=30000, use_shared_replay=True, use_qmix=False, use_vdn=False, use_tar2=False, config=None):
        self.num_episodes = num_episodes
        self.use_shared_replay = use_shared_replay
        self.use_vdn = use_vdn
        self.use_tar2 = use_tar2  # ä¿®æ­£: å¼•æ•°ã‹ã‚‰ç›´æ¥è¨­å®š
        self.use_qmix = use_qmix # è¿½åŠ 
        self.config = config
        
        # é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚’ä½¿ç”¨
        self.curriculum = Curriculum()
        
        self.agents = {}
        self.episode_rewards = {}
        self.avg_rewards = {}
        self.served_stats = {}

        # TAR2ç”¨ãƒãƒƒãƒ•ã‚¡ (è¿½åŠ )
        self.tar2 = None
        self.tar2_buffer = []
    
    def train(self):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        action_dim = 4
        
        # --- åˆæœŸç’°å¢ƒè¨­å®šï¼ˆçŠ¶æ…‹æ¬¡å…ƒå–å¾—ç”¨ï¼‰ ---
        # [cite_start]é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‹ã‚‰åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å–å¾— [cite: 68]
        current_stage = self.curriculum.get_current_stage()
        
        # ç’°å¢ƒåˆæœŸåŒ–
        current_env = RestaurantEnv(
            layout_type=current_stage['layout'],
            enable_customers=current_stage['customers'],
            customer_spawn_interval=current_stage['spawn_interval'],
            local_obs_size=5,
            config=self.config
        )

        state_dim = current_env.observation_space('agent_0').shape[0]

        algo_name = "Independent DQN"
        if self.use_qmix: algo_name = "QMIX"
        elif self.use_vdn: algo_name = "VDN"

        print(f"State Dimension: {state_dim}")
        print(f"System: {algo_name} | TAR2: {'ON' if self.use_tar2 else 'OFF'}")
        
        # TAR2 åˆæœŸåŒ–
        if self.use_tar2:
            self.tar2 = TAR2Network(state_dim, action_dim, num_agents=2)
            self.tar2_buffer = []

        # ãƒãƒƒãƒ•ã‚¡ãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        shared_buffer = SharedReplayBuffer(capacity=50000) if self.use_shared_replay else None
        
        if self.use_qmix:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã¯å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ã‚’çµåˆã—ãŸã‚‚ã®ã¨ã™ã‚‹ä¾‹
            global_state_dim = state_dim * 2 
            self.agents = {
                'qmix': QMIXAgent(state_dim, action_dim, global_state_dim, num_agents=2, shared_buffer=shared_buffer)
            }
        elif self.use_vdn:
            self.agents = {
                'vdn': VDNAgent(state_dim, action_dim, num_agents=2, shared_buffer=shared_buffer)
            }
        else:
            self.agents = {
                agent_name: DQNAgent(state_dim, action_dim, shared_buffer=shared_buffer) 
                for agent_name in current_env.possible_agents
            }
        
        self.episode_rewards = {agent: [] for agent in current_env.possible_agents}
        self.avg_rewards = {agent: [] for agent in current_env.possible_agents}
        self.served_stats = {agent: [] for agent in current_env.possible_agents}
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸æ»åœ¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        stage_episode_count = 0
        
        print(f"\n{'='*70}")
        print(f"=== STARTING STAGE: {current_stage['description']} ===")
        print(f"{'='*70}")
        
        for episode in range(self.num_episodes):
            
            # ----------------------------------------------------
            # 0. é©å¿œå‹ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã®é€²è¡Œåˆ¤å®š (é–¾å€¤/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)
            # ----------------------------------------------------
            # agent_0 ã®å¹³å‡å ±é…¬ã‚’ä»£è¡¨å€¤ã¨ã—ã¦ä½¿ç”¨
            current_served_performance = 0
            if len(self.served_stats['agent_0']) > 0:
                recent_indices = range(-min(50, len(self.served_stats['agent_0'])), 0)
                # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆè¨ˆã™ã‚‹
                total_served_list = [
                    sum(self.served_stats[agent][i] for agent in current_env.possible_agents)
                    for i in recent_indices
                ]
                current_served_performance = np.mean(total_served_list)
            else:
                current_served_performance = 0

            # check_progression ã«é…è†³æ•°ã®å¹³å‡ã‚’æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´ [cite: 143]
            should_proceed, reason = self.curriculum.check_progression(
                current_served_performance, 
                stage_episode_count
            )

            if should_proceed:
                # æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€
                new_stage = self.curriculum.get_current_stage()
                
                print(f"\n{'='*70}")
                print(f"ğŸ”„ CURRICULUM PROGRESSION")
                print(f"   From: {current_stage['description']}")
                print(f"   To:   {new_stage['description']}")
                print(f"   Why:  {reason}")
                print(f"   Perf: {current_served_performance:.1f} (Target: {current_stage['threshold']})")
                print(f"{'='*70}")
                
                # [cite_start]æ–°ã—ã„ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®šã§ç’°å¢ƒã‚’å†æ§‹ç¯‰ [cite: 79]
                current_stage = new_stage
                current_env = RestaurantEnv(
                    layout_type=current_stage['layout'],
                    enable_customers=current_stage['customers'],
                    customer_spawn_interval=current_stage['spawn_interval'],
                    local_obs_size=5,
                    coop_factor=0.5,
                    config=self.config
                )
                
                # æ»åœ¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
                stage_episode_count = 0
                
                # æ¢ç´¢ç‡(epsilon)ã®ãƒªã‚»ãƒƒãƒˆï¼ˆç’°å¢ƒãŒå¤‰ã‚ã£ãŸã®ã§å†æ¢ç´¢ã•ã›ã‚‹ï¼‰
                reset_epsilon = 0.8
                if self.use_vdn:
                    self.agents['vdn'].epsilon = max(self.agents['vdn'].epsilon, reset_epsilon)
                else:
                    for agent_name in self.agents:
                        self.agents[agent_name].epsilon = max(self.agents[agent_name].epsilon, reset_epsilon)

            # ----------------------------------------------------
            # 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º (å­¦ç¿’ãªã—ã§èµ°ã‚‰ã›ã‚‹)
            # ----------------------------------------------------
            trajectory_data = self._run_episode_collect_only(current_env)
            stage_episode_count += 1
            
            # å ±é…¬è¨˜éŒ²
            total_r = trajectory_data['total_reward']
            for agent_name in current_env.possible_agents:
                self.episode_rewards[agent_name].append(total_r / 2)
                self.avg_rewards[agent_name].append(np.mean(self.episode_rewards[agent_name][-50:]))
                self.served_stats[agent_name].append(current_env.served_count[agent_name])

            # ----------------------------------------------------
            # 2. TAR2 å ±é…¬å†è¨ˆç®—ãƒ•ã‚§ãƒ¼ã‚º
            # ----------------------------------------------------
            shaped_rewards = None
            if self.use_tar2:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                self.tar2_buffer.append(trajectory_data)
                
                # TAR2ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ (ãƒãƒƒãƒãŒæºœã¾ã£ãŸã‚‰)
                if len(self.tar2_buffer) >= 32:
                    b_states, b_actions, b_rewards, _ = collate_trajectories(self.tar2_buffer, self.tar2.device)
                    tar2_loss = self.tar2.update(b_states, b_actions, b_rewards)
                    self.tar2_buffer = []

                # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å ±é…¬ã‚’å†åˆ†é… (æ¨è«–)
                s, a, r_tot, _ = collate_trajectories([trajectory_data], self.tar2.device)
                f_s = s[:, -1, :, :]
                
                with torch.no_grad():
                    scores, _ = self.tar2(s, a, f_s)
                    shaped_tensor = self.tar2.get_redistributed_rewards(scores, r_tot)
                    shaped_rewards = shaped_tensor.squeeze(0).cpu().numpy() # (T, N)
            
            # ----------------------------------------------------
            # 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º (å†è¨ˆç®—ã•ã‚ŒãŸå ±é…¬ã‚’ä½¿ç”¨)
            # ----------------------------------------------------
            self._store_and_train_agents(trajectory_data, shaped_rewards)

            # Epsilonæ¸›è¡°
            if self.use_vdn:
                self.agents['vdn'].decay_epsilon()
            elif getattr(self, 'use_qmix', False): # ã“ã“ã‚’è¿½åŠ 
                self.agents['qmix'].decay_epsilon()
            else:
                for agent_name in current_env.possible_agents:
                    self.agents[agent_name].decay_epsilon()
            
            # [cite_start]å®šæœŸæ›´æ–° [cite: 90]
            # å®šæœŸæ›´æ–°
            if episode % 10 == 0:
                if self.use_vdn:
                    self.agents['vdn'].update_target_network()
                elif getattr(self, 'use_qmix', False): # è¿½åŠ 
                    self.agents['qmix'].update_target_network()
                else:
                    for agent_name in current_env.possible_agents:
                        self.agents[agent_name].update_target_network()
            
            # ãƒ­ã‚°è¡¨ç¤º
            if episode % 100 == 0:
                # 1. å¹³å‡å ±é…¬ï¼ˆç›´è¿‘50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç§»å‹•å¹³å‡ï¼‰ã®å–å¾— 
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã®å¹³å‡å ±é…¬ã‚’åˆè¨ˆã—ã¦ãƒãƒ¼ãƒ å…¨ä½“ã®æˆæœã¨ã™ã‚‹
                avg_0 = self.avg_rewards['agent_0'][-1] if self.avg_rewards['agent_0'] else 0
                avg_1 = self.avg_rewards['agent_1'][-1] if self.avg_rewards['agent_1'] else 0
                team_avg_reward = avg_0 + avg_1

                # 2. é…è†³æ•°ã®çµ±è¨ˆï¼ˆç›´è¿‘50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ 
                served_a0 = np.mean(self.served_stats['agent_0'][-50:])
                served_a1 = np.mean(self.served_stats['agent_1'][-50:])
                total_served = served_a0 + served_a1
                
                # 3. æ¢ç´¢ç‡ã¨TAR2ã®çŠ¶æ…‹å–å¾—
                if self.use_vdn:
                    eps = self.agents['vdn'].epsilon
                elif getattr(self, 'use_qmix', False):
                    eps = self.agents['qmix'].epsilon
                else:
                    eps = self.agents['agent_0'].epsilon
                tar2_msg = " | TAR2 Shaped" if self.use_tar2 else ""
                
                # ãƒ­ã‚°è¡¨ç¤ºã®æ›´æ–°
                print(f"Ep {episode:4d} | StgEp: {stage_episode_count:4d} | "
                      f"AvgReward: {team_avg_reward:6.1f} | "  # â† ã“ã“ã«å¾©æ´»ã•ã›ã¾ã—ãŸ
                      f"Total Served: {total_served:4.1f} (A0:{served_a0:.1f}, A1:{served_a1:.1f}) | "
                      f"Îµ={eps:.3f}{tar2_msg}")
        
        return self.agents, self.episode_rewards, self.avg_rewards, self.served_stats, current_env

    def _run_episode_collect_only(self, env):
        """å­¦ç¿’ã‚’è¡Œã‚ãšã€å…¨ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦è¿”ã™"""
        env.reset()
        
        states_seq = []
        actions_seq = []
        rewards_seq = []
        dones_seq = []
        global_states_seq = []  # QMIXç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹

        # [cite_start]ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ï¼ˆå…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬çµåˆï¼‰ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•° [cite: 387]
        def get_global_state(obs_dict):
            return np.concatenate([obs_dict[a] for a in env.possible_agents])

        # [cite_start]åˆæœŸè¦³æ¸¬ã®å–å¾— [cite: 388]
        states = {agent: env.observe(agent) for agent in env.possible_agents}
        
        # [cite_start]åˆæœŸã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’å–å¾— [cite: 388]
        current_global_state = get_global_state(states)
        
        episode_reward_sum = 0
        agents_order = env.possible_agents 
        
        for step in range(600): 
            step_states = []
            step_actions = []
            step_rewards = []
            step_dones = []
            
            current_actions = {} 
            for agent_name in agents_order: 
                state = states[agent_name]
                step_states.append(state)
                
                # [cite_start]è¡Œå‹•é¸æŠ [cite: 390, 391, 392, 393]
                if env.truncations.get(agent_name, False):
                    action = 0
                else:
                    if self.use_vdn:
                        actions_dict = self.agents['vdn'].select_actions(states)
                        action = actions_dict[agent_name]
                    elif getattr(self, 'use_qmix', False): # QMIXå¯¾å¿œ
                        actions_dict = self.agents['qmix'].select_actions(states)
                        action = actions_dict[agent_name]
                    else:
                        action = self.agents[agent_name].select_action(state)
                
                current_actions[agent_name] = action
                step_actions.append(action)

            # [cite_start]ç’°å¢ƒã®æ›´æ–° [cite: 394]
            for agent_name in agents_order:
                if env.agent_selection == agent_name:
                    env.step(current_actions[agent_name])
            
            # [cite_start]æ¬¡ã®çŠ¶æ…‹ã®è¦³æ¸¬ã¨å ±é…¬è¨˜éŒ² [cite: 395, 396]
            for agent_name in agents_order:
                next_obs = env.observe(agent_name)
                states[agent_name] = next_obs
                
                r = env.rewards.get(agent_name, 0)
                d = env.truncations.get(agent_name, False)
                
                step_rewards.append(r)
                step_dones.append(d)
                episode_reward_sum += r

            # [cite_start]ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æƒ…å ±ã‚’ä¿å­˜ã—ã€æ¬¡ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’æ›´æ–° [cite: 388]
            global_states_seq.append(current_global_state)
            current_global_state = get_global_state(states)

            states_seq.append(np.array(step_states))
            actions_seq.append(np.array(step_actions))
            rewards_seq.append(np.array(step_rewards))
            dones_seq.append(np.array(step_dones))

            if all(env.truncations.values()): 
                break
        
        return {
            'states': np.array(states_seq),
            'actions': np.array(actions_seq),
            'rewards': np.array(rewards_seq),
            'dones': np.array(dones_seq),
            'global_states': np.array(global_states_seq), # å­¦ç¿’ã«ä½¿ç”¨ [cite: 398]
            'total_reward': episode_reward_sum
        }

    def _store_and_train_agents(self, trajectory, shaped_rewards):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨å ±é…¬ã‚’ä½¿ã£ã¦ãƒãƒƒãƒ•ã‚¡ä¿å­˜ã¨å­¦ç¿’ã‚’è¡Œã†"""
        T = len(trajectory['states'])
        agents_order = ['agent_0', 'agent_1']
        
        for t in range(T - 1):
            s_t = trajectory['states'][t]
            a_t = trajectory['actions'][t]
            ns_t = trajectory['states'][t+1]
            d_t = trajectory['dones'][t]

            # QMIXç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
            g_s_t = trajectory['global_states'][t]
            g_ns_t = trajectory['global_states'][t+1]
            
            if shaped_rewards is not None:
                r_t = shaped_rewards[t]
            else:
                r_t = trajectory['rewards'][t]

            s_dict = {name: s_t[i] for i, name in enumerate(agents_order)}
            a_dict = {name: a_t[i] for i, name in enumerate(agents_order)}
            r_dict = {name: r_t[i] for i, name in enumerate(agents_order)}
            ns_dict = {name: ns_t[i] for i, name in enumerate(agents_order)}
            d_dict = {name: bool(d_t[i]) for i, name in enumerate(agents_order)}

            # --- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã”ã¨ã«æ’ä»–çš„ã«å®Ÿè¡Œã™ã‚‹ã‚ˆã†æ•´ç† ---
            if getattr(self, 'use_qmix', False):
                # 1. QMIXã®å ´åˆ
                self.agents['qmix'].store_transition(s_dict, a_dict, r_dict, ns_dict, d_dict, g_s_t, g_ns_t)
                self.agents['qmix'].train()
            elif self.use_vdn:
                # 2. VDNã®å ´åˆ
                self.agents['vdn'].store_transition(s_dict, a_dict, r_dict, ns_dict, d_dict)
                self.agents['vdn'].train()
            else:
                # 3. ç‹¬ç«‹DQNã®å ´åˆ
                for i, name in enumerate(agents_order):
                    self.agents[name].store_transition(
                        s_dict[name], a_dict[name], r_dict[name], ns_dict[name], d_dict[name]
                    )
                    self.agents[name].train()